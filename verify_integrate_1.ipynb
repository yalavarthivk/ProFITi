{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shiesh and SITA functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Shiesh(x):\n",
    "    return torch.arcsinh(np.exp(1)*torch.sinh(x))\n",
    "    \n",
    "def DShiesh(x):\n",
    "    den = 1+(np.exp(1)*torch.sinh(x))**2\n",
    "    jac = np.exp(1)*torch.cosh(x)/(den**0.5)\n",
    "    return jac\n",
    "\n",
    "def activation(x):\n",
    "    asd = torch.abs(x) > 30\n",
    "    int_val = torch.zeros_like(x)\n",
    "    jac = torch.ones_like(x)\n",
    "    int_val[asd] = x[asd] + torch.sign(x[asd])\n",
    "    int_val[~asd] = Shiesh(x[~asd])\n",
    "    jac[~asd] = DShiesh(x[~asd])\n",
    "    return int_val.to(torch.float32), jac\n",
    "\n",
    "def sita(K, scores):\n",
    "    id_tensor = torch.eye(K).to(scores.dtype)\n",
    "    A = scores.masked_fill(id_tensor == 1, 0)\n",
    "    B = scores.masked_fill(id_tensor == 0, -1e8)\n",
    "    A += torch.nn.Softplus()(B)\n",
    "    A = torch.tril(A)\n",
    "    return A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{profiti}^{-1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProFITi_inv(x, A, theta, phi):\n",
    "    nlloss = nn.GaussianNLLLoss(full=True, reduction='none')\n",
    "    \n",
    "    LJD = torch.tensor([0.]) # initiating log det jacobian\n",
    "\n",
    "    # applying sita\n",
    "    x = torch.matmul(A, x[:,None])[:,0]\n",
    "    ljd = torch.log(torch.diagonal(A, dim1 = -2, dim2 = -1)).sum()\n",
    "    LJD += ljd\n",
    "\n",
    "    # applying elementwise transformation\n",
    "    x = x*theta + phi\n",
    "    LJD += torch.log(theta).sum()\n",
    "\n",
    "    # applying shiesh\n",
    "    x, ljd = activation(x)\n",
    "    LJD += torch.log(ljd).sum()\n",
    "\n",
    "    gnll = nlloss(torch.zeros_like(x), x, torch.ones_like(x)) # computing diagonal gaussian nll\n",
    "    nll = gnll.sum() - LJD # computing joint nll\n",
    "    density = torch.exp(-nll) # computing likelihood\n",
    "    return density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifying if joint densities integrate to 1.\n",
    "\n",
    "$x \\in \\mathbb{R}^2$ is uniformly sampled from (-50,50) ($x$ indicates observation space).\n",
    "\n",
    "We map $x$ to gaussian via flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9959])\n"
     ]
    }
   ],
   "source": [
    "cum_density = 0\n",
    "scores = torch.randn(2,2)\n",
    "A = sita(2, scores).to(torch.float32) # Triangular Attention \n",
    "theta = torch.rand(2) # while theta = exp(tanh()), I just took small positive value for experiment\n",
    "phi = torch.randn(2) # it is the offset\n",
    "# now integrating from -50 to 50 \n",
    "for i in np.linspace(-50,50,500):\n",
    "    for j in np.linspace(-50,50,500):\n",
    "        x = torch.tensor([i,j]).to(torch.float32)\n",
    "        cum_density += 0.2*0.2*ProFITi_inv(x, A, theta, phi)\n",
    "print(cum_density)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_260",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
